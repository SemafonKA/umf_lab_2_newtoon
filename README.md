# 2 лаба по УМФ

Вариант 5:

Эллиптическое нелинейное уравнение:

$$
-div \left(\lambda\left(u, x\right) \ grad(u(x, t)) \right) + \sigma(x) \frac{\partial u}{\partial t} = f(x,t), \tag{1}
$$

$$
\begin{gather}
   \lambda = \lambda\left(u, x\right), \\
   \sigma = \sigma(x), \\
   f = f(x, t), \\
   u = u(x, t).
\end{gather}
$$

Далее кратенькие формулы по работе, подробная информация в отчёте.

## Матрица массы

$$
\hat{M}^{k} = \frac{\bar{\sigma} h_{x^{k}}}{6} \begin{pmatrix}
2 & 1 \\ 1  & 2
\end{pmatrix}
$$


## Матрица жёсткости

$$
\hat{G}^{k} = \frac{\lambda^{k}\left(u(x^{k},t), x^{k}\right) + \lambda^{k}\left(u(x^{k+1}, t), x^{k+1}\right)}{2h_{x^{k}}} \cdot 
\begin{pmatrix}
1  & -1 \\ -1 & 1
\end{pmatrix}
$$

## Вектор правой части

$$
\hat{b}^{k} = \hat{C} \cdot \{f(x^{k}, t), f(x^{k+1}, t)\} = \frac{h_{x^{k}}}{6} \begin{pmatrix}
2f(x^{k}, t) + f(x^{k+1}, t) \\[5px] f(x^{k}, t) + 2f(x^{k+1}, t)
\end{pmatrix}
$$

## Подбор лямбды

### Метод простых итераций

Алгоритм метода простых итераций:

1. Выбираем начальное приближение $q^{0}$
2. Подставляем его в $\lambda$ везде, считаем матрицу жёсткости
3. Собираем глобальную матрицу $A$ и вектор правой части $b$, решаем СЛАУ, находим $q^{1}$ (и $q^k$ далее)
4. Повторяем с п.2, пока не получим $\frac{\lVert A(q^{k})\cdot q^{k} - b\rVert}{\lVert b \rVert} < \varepsilon$

### Метод релаксации

Алгоритм метода релаксации будет таким же, как и алгоритм метода простых итераций, за исключением шага 3. В методе релаксации, на шаге 3 после решения СЛАУ мы получим $q^{k*}$, а вектор $q^k$ будем находить по следующей формуле:

$$
q^{k} = \omega q^{k*} + (1 - \omega) q^{k-1},
$$

где $\omega \in (0; 1]$.

### Метод Ньютона

Для удобства, введём понятие добавочной матрицы $\hat{A}^{*}$ к линеаризованной матрице $\hat{A}^{L}$:

$$
\hat{A}^{*}_{ij} = \sum\limits_{k=1}^{2}\left.\frac{\partial \hat{A}_{ik}(\hat{q})}{\partial \hat{q}_{j}} \right|_{\hat{q}=\hat{q}^{0}} \hat{q}^{0}_{k}
$$

Тогда матрица $\hat{A}^{L}_{ij}$ будет считаться следующим образом:

$$
\hat{A}^{L}_{ij} = \hat{A}_{ij} + \hat{A}^{*}_{ij},
$$

а вектор правой части $\hat{b}^{L}_{i}$ можно будет найти следующим образом:

$$
\hat{b}^{L}_{i} = \hat{b}_{i} + \hat{q}^{0}_{1}\cdot \hat{A}^{*}_{i1} + \hat{q}^{0}_{2}\cdot \hat{A}^{*}_{i2}
$$

Итерационный процесс совпадает с методом простых итераций. В том числе и условие выхода из итерационного процесса.

### Релаксация метода Ньютона

Метод Ньютона работает особенно хорошо, если подбирать параметр релаксации на каждой итерации. В целом, его можно подбирать при помощи одномерного поиска, минимизируя относительную невязку решения, но можно подбирать и следующим способом:

1. Получаем решение и относительную невязку этого решения при $\omega=1$
2. Если относительная невязка полученного решения оказалась больше, чем на прошлой итерации, то 
   1. Уменьшаем коэффициент релаксации на $0.1$
   2. Пересчитываем решение и его относительную невязку
   3. Если относительная невязка всё ещё больше требуемого, то возвращаемся в п. 2.1.

   ### Демпфирование метода Ньютона 

Как сказано в учебнике на странице 837, матрица $A^{L}$ может перестать быть положительно определённой из-за добавочной матрицы. Поэтому вводят так называемый *коэффициент демпфирования* $\nu \in (0; 1]$, который ставится перед добавками следующим образом:

$$
A^{L} = A + \nu \cdot A^{*},
$$

$$
b^{L} = b + \nu \cdot b^{*}.
$$

На каждом шаге итерационного процесса Ньютона коэффициент $\nu$ берётся равным единице, затем проверяется: если после решения СЛАУ погрешность возросла (даже после перебора параметров релаксации вплоть до 0.1), то коэффициент уменьшают (например, можно уменьшать его в 1.5-2 раза) и пересчитывают матрицы и вектор. На следующей итерации коэффициент можно снова приравнять единице.

### Вычисление производных для метода Ньютона

$$
\begin{multline}
\hat{A}^{*}_{ij} = \sum\limits_{k=1}^{2}\left.\frac{\partial \hat{A}_{ik}(\hat{q})}{\partial \hat{q}_{j}} \right|_{\hat{q}=\hat{q}^{0}} \hat{q}^{0}_{k} = \sum\limits_{k=1}^{2}\left.\frac{\partial \hat{G}_{ik}(\hat{q})}{\partial \hat{q}_{j}} \right|_{\hat{q}=\hat{q}^{0}} \hat{q}^{0}_{k} = \\
= \left.\frac{\partial \hat{G}_{i1}(\hat{q})}{\partial \hat{q}_{j}} \right|_{\hat{q}=\hat{q}^{0}} \hat{q}^{0}_{1} + \left.\frac{\partial \hat{G}_{i2}(\hat{q})}{\partial \hat{q}_{j}} \right|_{\hat{q}=\hat{q}^{0}} \hat{q}^{0}_{2}
\end{multline}
$$

## Аппроксимация по времени

Аппроксимацию по времени для  нашего параболического уравнения будем проводить по двухслойной неявной схеме:

$$
\left. \frac{\partial u}{\partial t} \right|_{t=t_{s}} = \frac{u_{s} - u_{s-1}}{\Delta t_s}
$$

Таким образом, решение нашей задачи $(1)$ сводится к решению следующей задачи:

$$
-div \left(\lambda\left(u(x, t_{s}), x\right) \ grad(u(x, t_{s})) \right) + \sigma(x) \frac{u(x,t_{s})}{\Delta t_{s}} = f(x,t_s) + \sigma(x)\frac{u(x,t_{s-1})}{\Delta t_{s}}
$$

Это уравнение, после конечномерной аппроксимации, превратится в следующее:

$$
A(q_{s})q_{s} = d,
$$

где 

$$
A(q_{s}) = G(q_{s}) + \frac{1}{\Delta t_s} M_{\sigma},
$$

$$
d = b_{s} + \frac{1}{\Delta t} M_{\sigma}\cdot q_{s-1},
$$

$$
b_{s} = M_{\sigma} |_{\sigma = 1} \cdot \bar{f}(x, t_s).
$$


